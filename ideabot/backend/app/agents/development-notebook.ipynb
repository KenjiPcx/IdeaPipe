{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from typing import AsyncGenerator, List, Optional\n",
    "\n",
    "# Import our agent team\n",
    "from app.agents.a_problem_definition.problem_validator import ProblemValidatorFeedback, problem_validator_prompt_instructions\n",
    "from app.agents.b_initial_research import create_market_research, create_competitor_analysis, create_customer_insights, create_online_trends\n",
    "from app.agents.c_research_reviewer import create_research_reviewer\n",
    "from app.agents.d_feasibility_research import create_finance_feasibility, create_operations_feasibility, create_tech_feasibility\n",
    "from app.agents.e_strategy_research import create_go_to_market, create_monetization, create_risk_analysis\n",
    "from app.agents.f_output_production import create_landing_page_poc, create_podcaster, create_summarizer\n",
    "\n",
    "from app.workflows.single import AgentRunEvent, AgentRunResult, FunctionCallingAgent\n",
    "from llama_index.core.chat_engine.types import ChatMessage\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### High Level Events ###\n",
    "class QnaWorkflowEvent(Event):\n",
    "    '''\n",
    "    Fired when the user has a question for the AI\n",
    "    '''\n",
    "    input: str\n",
    "\n",
    "class ResearchWorkflowEvent(Event):\n",
    "    '''\n",
    "    Fired when the user has a research query\n",
    "    '''\n",
    "    input: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initial Research Events ###\n",
    "class StartResearchPipelineEvent(Event):\n",
    "    '''\n",
    "    Fired when the user has provided enough information to start the research pipeline\n",
    "    '''\n",
    "    input: str\n",
    "    \n",
    "class InitialResearchCompleteEvent(Event):\n",
    "    '''\n",
    "    Fired when the initial research is complete\n",
    "    '''\n",
    "    input: str\n",
    "\n",
    "class CompetitorAnalysisFeedbackEvent(Event):\n",
    "    '''\n",
    "    Fired when the competitor analysis critique gives feedback\n",
    "    '''\n",
    "    input: str\n",
    "\n",
    "class GetCompetitorAnalysisCritiqueEvent(Event):\n",
    "    '''\n",
    "    Fired when the competitor analysis agent needs feedback\n",
    "    '''\n",
    "    input: str\n",
    "\n",
    "class GetCustomerInsightsCritiqueEvent(Event):\n",
    "    '''\n",
    "    Fired when the customer insights agent needs feedback\n",
    "    '''\n",
    "    input: str\n",
    "\n",
    "class CustomerInsightsFeedbackEvent(Event):\n",
    "    '''\n",
    "    Fired when the customer insights critique gives feedback\n",
    "    '''\n",
    "    input: str\n",
    "\n",
    "class GetOnlineTrendsCritiqueEvent(Event):\n",
    "    '''\n",
    "    Fired when the online trends agent needs feedback\n",
    "    '''\n",
    "    input: str\n",
    "\n",
    "class OnlineTrendsFeedbackEvent(Event):\n",
    "    '''\n",
    "    Fired when the online trends critique gives feedback\n",
    "    '''\n",
    "    input: str\n",
    "\n",
    "class MarketResearchFeedbackEvent(Event):\n",
    "    '''\n",
    "    Fired when the market research critique gives feedback\n",
    "    '''\n",
    "    input: str\n",
    "\n",
    "class GetMarketResearchCritiqueEvent(Event):\n",
    "    '''\n",
    "    Fired when the market research agent needs feedback\n",
    "    '''\n",
    "    input: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feasibility Research Events ###\n",
    "class StartFeasibilityResearchEvent(Event):\n",
    "    '''\n",
    "    Fired when the user has provided enough information to start the feasibility research pipeline\n",
    "    '''\n",
    "    input: str\n",
    "\n",
    "class FeasibilityCompleteEvent(Event):\n",
    "    '''\n",
    "    Fired when the feasibility research is complete\n",
    "    '''\n",
    "    input: str\n",
    "\n",
    "class TechFeasibilityFeedbackEvent(Event):\n",
    "    '''\n",
    "    Fired when the tech feasibility critique gives feedback\n",
    "    '''\n",
    "    input: str\n",
    "\n",
    "class GetTechFeasibilityCritiqueEvent(Event):\n",
    "    '''\n",
    "    Fired when the tech feasibility research agent needs feedback\n",
    "    '''\n",
    "    input: str\n",
    "\n",
    "class FinanceFeasibilityFeedbackEvent(Event):\n",
    "    '''\n",
    "    Fired when the finance feasibility critique gives feedback\n",
    "    '''\n",
    "    input: str\n",
    "\n",
    "class GetFinanceFeasibilityCritiqueEvent(Event):\n",
    "    '''\n",
    "    Fired when the finance feasibility research agent needs feedback\n",
    "    '''\n",
    "    input: str  \n",
    "\n",
    "class OperationsFeasibilityFeedbackEvent(Event):\n",
    "    '''\n",
    "    Fired when the operations feasibility critique gives feedback\n",
    "    '''\n",
    "    input: str\n",
    "\n",
    "class GetOperationsFeasibilityCritiqueEvent(Event):\n",
    "    '''\n",
    "    Fired when the operations feasibility research agent needs feedback\n",
    "    '''\n",
    "    input: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskAnalysisCompleteEvent(Event):\n",
    "    '''\n",
    "    Fired when the risk analysis is complete\n",
    "    '''\n",
    "    input: str\n",
    "\n",
    "class GetRiskAnalysisCritiqueEvent(Event):\n",
    "    '''\n",
    "    Fired when the risk analysis agent needs feedback\n",
    "    '''\n",
    "    input: str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizeEverythingEvent(Event):\n",
    "    '''\n",
    "    Fired when all research is complete and that its time to summarize everything\n",
    "    '''\n",
    "    input: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class IdeaResearchWorkflow(Workflow):\n",
    "    def __init__(\n",
    "        self, timeout: int = 600, chat_history: Optional[List[ChatMessage]] = None\n",
    "    ):\n",
    "        super().__init__(timeout=timeout)\n",
    "        self.chat_history = chat_history or []\n",
    "\n",
    "    @step()\n",
    "    async def start(self, ctx: Context, ev: StartEvent) -> QnaWorkflowEvent | ResearchWorkflowEvent:\n",
    "        # set streaming\n",
    "        ctx.data[\"streaming\"] = getattr(ev, \"streaming\", False)\n",
    "        # start the workflow with researching about a topic\n",
    "        ctx.data[\"task\"] = ev.input\n",
    "        ctx.data[\"user_input\"] = ev.input\n",
    "\n",
    "        # Decision-making process\n",
    "        prompt_template = PromptTemplate(\n",
    "            dedent(\n",
    "                \"\"\"\n",
    "                ### High Level Context\n",
    "                You are part of a workflow to help users do autonomous research and idea validation for their business ideas.\n",
    "\n",
    "                ### Your Role\n",
    "                You are an expert in decision-making, given the chat history and the new user request, decide whether the user request is a research query (including providing more information for an existing research query) or a question for the AI.\n",
    "\n",
    "                Here is the chat history:\n",
    "                {chat_history}\n",
    "\n",
    "                The current user request is:\n",
    "                {input}\n",
    "\n",
    "                Decision (respond with either 'research' or 'qna'):\n",
    "            \"\"\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        chat_history_str = \"\\n\".join(\n",
    "            [f\"{msg.role}: {msg.content}\" for msg in self.chat_history]\n",
    "        )\n",
    "        prompt = prompt_template.format(chat_history=chat_history_str, input=ev.input)\n",
    "\n",
    "        output = await Settings.llm.acomplete(prompt)\n",
    "        decision = output.text.strip().lower()\n",
    "\n",
    "        if decision == \"research\":\n",
    "            return ResearchWorkflowEvent(input=f\"User input: {ev.input}\")\n",
    "        else:\n",
    "            return QnaWorkflowEvent(input=f\"User input: {ev.input}\")\n",
    "\n",
    "    @step()\n",
    "    async def validate_problem_statement(self, ctx: Context, ev: ResearchWorkflowEvent) -> StartResearchPipelineEvent | StopEvent:\n",
    "        chat_history_str = \"\\n\".join(\n",
    "            [f\"{msg.role}: {msg.content}\" for msg in self.chat_history]\n",
    "        )\n",
    "        prompt = problem_validator_prompt_instructions.format(chat_history=chat_history_str, input=ev.input)\n",
    "\n",
    "        output = await Settings.llm.as_structured_llm(output_cls=ProblemValidatorFeedback).acomplete(prompt)\n",
    "        result: ProblemValidatorFeedback = output.raw\n",
    "        if result.enough_information:\n",
    "            return StartResearchPipelineEvent(input=result.refined_problem_statement)\n",
    "        else:\n",
    "            return StopEvent(input=result.feedback)\n",
    "\n",
    "    @step()\n",
    "    async def qna(self, ctx: Context, ev: QnaWorkflowEvent) -> StopEvent:\n",
    "        messages = self.chat_history + [ChatMessage(role=\"user\", content=ev.input)]\n",
    "        result = await Settings.llm.achat(messages)\n",
    "        return StopEvent(result=result.content)\n",
    "    \n",
    "    ### Initial Research Analysts Team 1 ###\n",
    "\n",
    "    ### Market Research ###\n",
    "    @step()\n",
    "    async def market_research(self, ctx: Context, ev: StartResearchPipelineEvent | MarketResearchFeedbackEvent) -> GetMarketResearchCritiqueEvent:\n",
    "        return GetMarketResearchCritiqueEvent(input=ev.input)\n",
    "\n",
    "    @step()\n",
    "    async def critique_market_research(self, ctx: Context, ev: GetMarketResearchCritiqueEvent) -> InitialResearchCompleteEvent | MarketResearchFeedbackEvent:\n",
    "        if random.random() > 0.5:\n",
    "            return InitialResearchCompleteEvent(input=ev.input)\n",
    "        else:\n",
    "            return MarketResearchFeedbackEvent(input=ev.input)\n",
    "\n",
    "    ### Customer Insights ###\n",
    "    @step()\n",
    "    async def customer_insights(self, ctx: Context, ev: StartResearchPipelineEvent | CustomerInsightsFeedbackEvent) -> GetCustomerInsightsCritiqueEvent:\n",
    "        return GetCustomerInsightsCritiqueEvent(input=ev.input)\n",
    "\n",
    "    @step()\n",
    "    async def critique_customer_insights(self, ctx: Context, ev: GetCustomerInsightsCritiqueEvent) -> InitialResearchCompleteEvent | CustomerInsightsFeedbackEvent:\n",
    "        if random.random() > 0.5:\n",
    "            return InitialResearchCompleteEvent(input=ev.input)\n",
    "        else:\n",
    "            return CustomerInsightsFeedbackEvent(input=ev.input)    \n",
    "\n",
    "    ### Online Trends ###\n",
    "    @step()\n",
    "    async def online_trends(self, ctx: Context, ev: StartResearchPipelineEvent | OnlineTrendsFeedbackEvent) -> GetOnlineTrendsCritiqueEvent:\n",
    "        return GetOnlineTrendsCritiqueEvent(input=ev.input)\n",
    "\n",
    "    @step()\n",
    "    async def critique_online_trends(self, ctx: Context, ev: GetOnlineTrendsCritiqueEvent) -> InitialResearchCompleteEvent | OnlineTrendsFeedbackEvent:\n",
    "        if random.random() > 0.5:\n",
    "            return InitialResearchCompleteEvent(input=ev.input)\n",
    "        else:\n",
    "            return OnlineTrendsFeedbackEvent(input=ev.input)\n",
    "\n",
    "    ### Competitor Analysis ###\n",
    "    @step()\n",
    "    async def competitor_analysis(self, ctx: Context, ev: StartResearchPipelineEvent | CompetitorAnalysisFeedbackEvent) -> GetCompetitorAnalysisCritiqueEvent:\n",
    "        return GetCompetitorAnalysisCritiqueEvent(input=ev.input)\n",
    "\n",
    "    @step()\n",
    "    async def critique_competitor_analysis(self, ctx: Context, ev: GetCompetitorAnalysisCritiqueEvent) -> InitialResearchCompleteEvent | CompetitorAnalysisFeedbackEvent:\n",
    "        if random.random() > 0.5:\n",
    "            return InitialResearchCompleteEvent(input=ev.input)\n",
    "        else:\n",
    "            return CompetitorAnalysisFeedbackEvent(input=ev.input)\n",
    "\n",
    "    ### Collect Initial Research Feedback ###\n",
    "    @step()\n",
    "    async def review_initial_research(self, ctx: Context, ev: InitialResearchCompleteEvent) -> SummarizeEverythingEvent:\n",
    "        print(\"Received event \", ev.result)\n",
    "\n",
    "        # wait until we receive all 4 prior events\n",
    "        if (\n",
    "            ctx.collect_events(\n",
    "                ev,\n",
    "                [InitialResearchCompleteEvent] * 4,\n",
    "            )\n",
    "            is None\n",
    "        ):\n",
    "            return None\n",
    "        return SummarizeEverythingEvent(input=ev.input)\n",
    "\n",
    "    ### Output Production ###\n",
    "    @step()\n",
    "    async def summarize_everything(self, ctx: Context, ev: SummarizeEverythingEvent) -> StopEvent:\n",
    "        return StopEvent(result=ev.input)\n",
    "\n",
    "    # ### Feasibility Research ###\n",
    "    # @step()\n",
    "    # async def tech_feasibility(self, ctx: Context, ev: StartFeasibilityResearchEvent) -> StopEvent:\n",
    "    #     return StopEvent(result=ev.input)\n",
    "\n",
    "    async def run_agent(\n",
    "        self,\n",
    "        ctx: Context,\n",
    "        agent: FunctionCallingAgent,\n",
    "        input: str,\n",
    "        streaming: bool = False,\n",
    "    ) -> AgentRunResult | AsyncGenerator:\n",
    "        handler = agent.run(input=input, streaming=streaming)\n",
    "        # bubble all events while running the executor to the planner\n",
    "        async for event in handler.stream_events():\n",
    "            # Don't write the StopEvent from sub task to the stream\n",
    "            if type(event) is not StopEvent:\n",
    "                ctx.write_event_to_stream(event)\n",
    "        return await handler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class '__main__.GetCompetitorAnalysisCritiqueEvent'>\n",
      "<class '__main__.InitialResearchCompleteEvent'>\n",
      "<class '__main__.CompetitorAnalysisFeedbackEvent'>\n",
      "<class '__main__.InitialResearchCompleteEvent'>\n",
      "<class '__main__.CustomerInsightsFeedbackEvent'>\n",
      "<class '__main__.InitialResearchCompleteEvent'>\n",
      "<class '__main__.MarketResearchFeedbackEvent'>\n",
      "<class '__main__.InitialResearchCompleteEvent'>\n",
      "<class '__main__.OnlineTrendsFeedbackEvent'>\n",
      "<class '__main__.GetCustomerInsightsCritiqueEvent'>\n",
      "<class '__main__.GetMarketResearchCritiqueEvent'>\n",
      "<class '__main__.GetOnlineTrendsCritiqueEvent'>\n",
      "<class 'llama_index.core.workflow.events.StopEvent'>\n",
      "<class '__main__.SummarizeEverythingEvent'>\n",
      "<class '__main__.QnaWorkflowEvent'>\n",
      "<class '__main__.ResearchWorkflowEvent'>\n",
      "<class 'llama_index.core.workflow.events.StopEvent'>\n",
      "<class '__main__.StartResearchPipelineEvent'>\n",
      "<class 'llama_index.core.workflow.events.StopEvent'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workflow_all_flows.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(IdeaResearchWorkflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "WorkflowValidationError",
     "evalue": "The following events are produced but never consumed: StartFeasibilityResearchEvent, MarketResearchFeedbackEvent",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWorkflowValidationError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m workflow \u001b[38;5;241m=\u001b[39m IdeaResearchWorkflow(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m360\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms LlamaIndex?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m result\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/app-zBMyfuJ1-py3.11/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:311\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    314\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/app-zBMyfuJ1-py3.11/lib/python3.11/site-packages/llama_index/core/workflow/workflow.py:326\u001b[0m, in \u001b[0;36mWorkflow.run\u001b[0;34m(self, ctx, stepwise, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the workflow until completion.\"\"\"\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# Validate the workflow if needed\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m uses_hitl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m uses_hitl \u001b[38;5;129;01mand\u001b[39;00m stepwise:\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WorkflowRuntimeError(\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuman-in-the-loop is not supported with stepwise execution\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    330\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/app-zBMyfuJ1-py3.11/lib/python3.11/site-packages/llama_index/core/workflow/workflow.py:456\u001b[0m, in \u001b[0;36mWorkflow._validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_events:\n\u001b[1;32m    455\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ev\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ev \u001b[38;5;129;01min\u001b[39;00m unused_events)\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WorkflowValidationError(\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following events are produced but never consumed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnames\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    458\u001b[0m     )\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Check all the requested services are available\u001b[39;00m\n\u001b[1;32m    461\u001b[0m required_service_names \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    462\u001b[0m     sd\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m sd \u001b[38;5;129;01min\u001b[39;00m requested_services \u001b[38;5;28;01mif\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mdefault_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    463\u001b[0m }\n",
      "\u001b[0;31mWorkflowValidationError\u001b[0m: The following events are produced but never consumed: StartFeasibilityResearchEvent, MarketResearchFeedbackEvent"
     ]
    }
   ],
   "source": [
    "workflow = IdeaResearchWorkflow(timeout=360)\n",
    "result = await workflow.run(input=\"What's LlamaIndex?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    InputRequiredEvent,\n",
    "    HumanResponseEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"\"\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from typing import List, Optional\n",
    "from llama_index.core.settings import Settings\n",
    "from app.agents.new_workflow import StartResearchPipelineEvent\n",
    "from llama_cloud import ChatMessage\n",
    "from app.agents.a_problem_definition.problem_validator import problem_definer_prompt_instructions, ProblemDefinerFeedback\n",
    "\n",
    "class TestEvent(Event):\n",
    "    input: str\n",
    "\n",
    "class OpenAIGenerator(Workflow):\n",
    "    def __init__(\n",
    "        self, timeout: int = 600, chat_history: Optional[List[ChatMessage]] = None\n",
    "    ):\n",
    "        super().__init__(timeout=timeout)\n",
    "        self.chat_history = chat_history or []\n",
    "        \n",
    "    @step\n",
    "    async def generate(self, ev: StartEvent) -> TestEvent | StartResearchPipelineEvent:\n",
    "        # Validate user problem statement\n",
    "        prompt_template = problem_definer_prompt_instructions\n",
    "        chat_history_str = \"\\n\".join(\n",
    "            [f\"{msg.role}: {msg.content}\" for msg in self.chat_history]\n",
    "        )\n",
    "        prompt = prompt_template.format(chat_history=chat_history_str, input=\"TEST\")\n",
    "\n",
    "        output = await OpenAI(model=\"gpt-4o-mini\", api_key=API_KEY).as_structured_llm(output_cls=ProblemDefinerFeedback).acomplete(prompt)\n",
    "        res: ProblemDefinerFeedback = output.raw\n",
    "        \n",
    "        if res.enough_information:\n",
    "            return StartResearchPipelineEvent(input=res.refined_problem_statement)\n",
    "        else:\n",
    "            return TestEvent(input=res.feedback)\n",
    "        \n",
    "    @step\n",
    "    async def step2(self, ev: TestEvent) -> StopEvent:\n",
    "        return StopEvent(result=ev.input)\n",
    "    \n",
    "    @step\n",
    "    async def step3(self, ev: StartResearchPipelineEvent) -> StopEvent:\n",
    "        return StopEvent(result=ev.input)\n",
    "\n",
    "\n",
    "w = OpenAIGenerator(timeout=10)\n",
    "handler = w.run(query=\"What's LlamaIndex?\")\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, InputRequiredEvent):\n",
    "        # here, we can handle human input however you want\n",
    "        # this means using input(), websockets, accessing async state, etc.\n",
    "        # here, we just use input()\n",
    "        response = input(event.prefix)\n",
    "        handler.ctx.send_event(HumanResponseEvent(response=response))\n",
    "\n",
    "await handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The input 'TEST' is too vague and does not provide any specific information about a problem, the affected individuals, or the impact on their lives. Please provide more detailed information about the business idea or problem you want to address.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "from llama_index.core.settings import Settings\n",
    "from app.agents.new_workflow import StartResearchPipelineEvent\n",
    "from llama_cloud import ChatMessage\n",
    "from app.agents.a_problem_definition.problem_validator import problem_definer_prompt_instructions, ProblemDefinerFeedback\n",
    "\n",
    "class IdeaResearcdWorkflow(Workflow):\n",
    "    def __init__(\n",
    "        self, timeout: int = 600, chat_history: Optional[List[ChatMessage]] = None\n",
    "    ):\n",
    "        super().__init__(timeout=timeout)\n",
    "        self.chat_history = chat_history or []\n",
    "        \n",
    "    @step\n",
    "    async def start(self, ctx: Context, ev: StartEvent) -> TestEvent | StartResearchPipelineEvent:\n",
    "        # Validate user problem statement\n",
    "        prompt_template = problem_definer_prompt_instructions\n",
    "        chat_history_str = \"\\n\".join(\n",
    "            [f\"{msg.role}: {msg.content}\" for msg in self.chat_history]\n",
    "        )\n",
    "        prompt = prompt_template.format(chat_history=chat_history_str, input=\"TEST\")\n",
    "\n",
    "        output = await OpenAI(model=\"gpt-4o-mini\", api_key=API_KEY).as_structured_llm(output_cls=ProblemDefinerFeedback).acomplete(prompt)\n",
    "        res: ProblemDefinerFeedback = output.raw\n",
    "        \n",
    "        if res.enough_information:\n",
    "            return StartResearchPipelineEvent(input=res.refined_problem_statement)\n",
    "        else:\n",
    "            return TestEvent(input=res.feedback)\n",
    "        \n",
    "    @step\n",
    "    async def step2(self, ev: TestEvent) -> StopEvent:\n",
    "        return StopEvent(result=ev.input)\n",
    "    \n",
    "    @step\n",
    "    async def step3(self, ev: StartResearchPipelineEvent) -> StopEvent:\n",
    "        return StopEvent(result=ev.input)\n",
    "\n",
    "    \n",
    "workflow = OpenAIGenerator(timeout=360)\n",
    "result = await workflow.run(query=\"What's LlamaIndex?\")\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-zBMyfuJ1-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
